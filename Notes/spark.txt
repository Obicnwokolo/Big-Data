SPARK
--------------------------------------------------------------------------------

#server: 18.134.132.202
--------------------------------------------------------------------------------------
pyspark --master local[*]
--------------------------------------------------------------------------------------
rdd=spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10])
rdd.collect()
--------------------------------------------------------------------------------------
rdd2 = spark.sparkContext.textFile("/tmp/bigdata_nov_2024/obinna/pyspark/Bank_Churn.csv")
rdd2.collect()
--------------------------------------------------------------------------------------
CREATING DATAFRAME
data = [('James','','Smith','1991-04-01','M',3000),
  ('Michael','Rose','','2000-05-19','M',4000),
  ('Robert','','Williams','1978-09-05','M',4000),
  ('Maria','Anne','Jones','1967-12-01','F',4000),
  ('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname","middlename","lastname","dob","gender","salary"]
df = spark.createDataFrame(data=data, schema = columns)
-----------------------------------------------------------------------------------------------
SPARK
Spark Architecture, an open-source, framework-based component that processes a large amount of unstructured, semi-structured, and structured data for analytics, is utilised in Apache Spark.

Features
-SPeed
-Powerful Caching
- Deployment
- Real-time
- Polygot


TWO MAIN ABSTRACTIONS
1. Resilient Distributed Datasets (RDD): It helps in recomputing data in case of failures, and it is a data structure. 
	There are two methods for modifying RDDs: 
		- transformations
		- actions.
2. Directed Acyclic Graph (DAG):The driver converts the program into a DAG for each job. A sequence of connection between nodes is referred to as a driver.

SPARK ECOSYSTEM
-Spark API core		-Spark SQL	 -Streaming and real-time processing, 
-Spark MLIB		-Spark Graph X

SPARK ARCHITECTURE
- Driver Program [Spark context]: Spark contexts (gateways) are created by the driver to monitor the job working in a specific cluster and to connect to a Spark cluster.
The Spark Context receives task information from the Cluster Manager and enqueues it on worker nodes.
- Cluster Manager: The Cluster Manager manages the execution of various jobs in the cluster.
- Worker node: The slave nodes function as executors, processing tasks, and returning the results back to the spark context. 
- Executors: Executors read and write external data in addition to servicing client requests.

MODES OF EXECUTION
- Cluster mode
- Client mode
- Local mode
------------------------------------------------------------------------------------
Narrow transformations: Examples include map, filter, and union. These can be processed within a single stage without needing to move data between partitions.
Wide transformations: Examples include groupByKey, reduceByKey, join, and distinct. These require a shuffle of data between partitions, which creates new stages in the DAG (Directed Acyclic Graph) and generally results in more processing overhead.
-------------------------------------------------------------------------------------

data = [("apple", 1), ("banana", 2), ("apple", 3), ("banana", 4)]

# Create an RDD
rdd = sc.parallelize(data)

# Perform a wide transformation: groupByKey (this involves a shuffle)
grouped_rdd = rdd.groupByKey()

# Collect the result (this will trigger the action)
result = grouped_rdd.collect()

# Print the result
print(result)
--------------------------------------------------------------------------------------
18.134.132.202:<4044> >> To access the spark web UI
----------------------------------------------------------------------------------------
export SPARK_HOME=//opt/cloudera/parcels/CDH/lib/spark
export PATH=$SPARK_HOME/bin:$PATH
--------------------------------------------------------------------------------------
# Create DataFrame from CSV file
df= spark.read.option("header", "true").csv("C:/Users/chigb/Downloads/fraud_records.csv") : use header in file.

df.printSchema(): to get the schema of the DataFrame

df.show() : To display the DataFrame. which shows the 20 rows by default.
---------------------------------------------------------------------------------------------------------
#To rename DataFrame column name
df.withColumnRenamed("fraud_detected_date", "detection_date").printSc
hema()

#To rename multiple columns

-------------------------------------------------------------------------------------------------------------------
# To run SPARK SCALA
spark-shell

#Create RDD using scala
// Create a local collection (List, Array, etc.)
val data = List(1, 2, 3, 4, 5)

// Create an RDD from the collection
val rdd = sc.parallelize(data)

// Show the contents of the RDD
rdd.collect().foreach(println)
---------------------------------------------------------------------------------------------------------------
// Existing RDD
val rdd = sc.parallelize(List(1, 2, 3, 4, 5))

// Apply a transformation (filter)
val filteredRDD = rdd.filter(x => x % 2 == 0)

// Collect and print the results
filteredRDD.collect().foreach(println)
---------------------------------------------------------------------------------------------------------------------
// Create an RDD from a CSV file
val csvRdd = sc.textFile("C:/Users/chigb/Downloads/fraud_records.csv")

// Process CSV data (you might need to split columns, etc.)
csvRdd.map(line => line.split(",")).collect().foreach(arr => println(arr.mkString(" ")))
----------------------------------------------------------------------------------------------------------------------
// Create an RDD from a HDFS CSV file
val hdfsRdd = sc.textFile("/tmp/bigdata_nov_2024/obinna/data1.csv")
val hdfsRdd2 = sc.textFile("/tmp/bigdata_nov_2024/obinna/data.csv")

// Process HDFS CSV data (you might need to split columns, etc.)
hdfsRdd.map(line => line.split(",")).collect().foreach(arr => println(arr.mkString(" ")))


PYSPARK

Read the Csv file
>>> df1= spark.read.option("header", "true").csv("C:/Users/chigb/Download
s/Bank_Churn.csv")
>>> df1.show(5)

Filtered the data based on Gender(Male)
>>> filtered_df1 = df1.filter(df1["Gender"]== "Male")
>>> filtered_df1.show(5)

Added 2 New columns
>>> from pyspark.sql.functions import lit, length, when
>>> df1 = df1.withColumn("LenName", length(df1["Surname"]))
>>> df1 = df1.withColumn("LossType", when(df1["CreditScore"] >= 700, "Big loss").otherwise("loss"))
>>> df1.show(5)

Dropped LenName column
>>> df1= df1.drop("LenName")
>>> df1.show()

Changed COlumn name from Geography to Location
>>> df1 = df1.withColumnRenamed("Geography", "Location")

Group By NumOfProducts
>>> group_df= df1.groupBy("NumOfProducts").count()
>>> group_df.show()

Sort Table by CustomerId
>>> df1_sorted = df1.orderBy("CustomerId")
>>> df1_sorted.show()

Replace Values of Active members
>>> mod_df1 =df1.withColumn("IsActiveMember", when(df1["IsActiveMember"]== 1, "Yes").otherwise("No"))
>>> mod_df1.show()

Change CustomerId from String data type to integer
>>> df1= df1.withColumn("CustomerId", df1["CustomerId"].cast("int"))
df1.printSchema()

Normalize Expected Salary
>>> CS_mean= df1.agg(mean("EstimatedSalary")).collect()[0][0]
>>> CS_stddev= df1.agg(stddev("EstimatedSalary")).collect()[0][0]
>>> df1 = df1.withColumn("Standardaized Est_Salary", (col("EstimatedSalary") - CS_mean) / CS_stddev)
>>> df1.show()


Save CSV to machine

# READ FROM POSTGRESQL
 df = spark.read.format("jdbc").option("url", "jdbc:postgresql://18.132.73.146:5432/testdb").option("driver", "org.postgresql.Driver").option("dbtable", "person").option("user", "mailto:consultants").option("password","welcomeitc@2022").load()
l>>> df.printSchema();